{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a337539",
   "metadata": {},
   "source": [
    "# PR√ÅCTICA DE TERREMOTOS UTILIZANDO CASSANDRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c1765",
   "metadata": {},
   "source": [
    "En esta pr√°ctica se usar√° la base de datos NoSQL de Cassandra, para el caso de uso de modelaci√≥n de una serie de consultas.\n",
    "\n",
    "En el caso de los datos s√≠smicos, el volumen, la variedad y, especialmente, la\n",
    "velocidad de generaci√≥n de la informaci√≥n (nuevos registros de sensores, actualizaciones de\n",
    "magnitudes, alertas de tsunami, datos geoespaciales y de profundidad) hacen que un enfoque\n",
    "relacional resulte poco eficiente para este caso de uso.\n",
    "\n",
    "A continuaci√≥n se observar√° el criterio de elecci√≥n de Cassandra como base de datos NoSQL para modelar las consultas, as√≠ como el modelo de datos de las distintas tablas, inserci√≥n y actualizaci√≥n de datos, y la creaci√≥n y ejecuci√≥n de las consultas SQL propuestas en la actividad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40907b55",
   "metadata": {},
   "source": [
    "## 1. Ventajas y desventajas de Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399576a",
   "metadata": {},
   "source": [
    "Antes de empezar con ventajas y desventajas, se debe de analizar el teorema CAP, para saber de forma general si Cassandra es de utilidad en nuestro caso de uso de Terremotos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfe1d12",
   "metadata": {},
   "source": [
    "### ===================\n",
    "### Teorema CAP en Cassandra\n",
    "### ==================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf521c32",
   "metadata": {},
   "source": [
    "* ¬øQue cumple simpre?:\n",
    "    * Tolerancia a particiones (P): En un entorno real, Cassandra usa varios nodos a modo de backup siguiendo una topolog√≠a de anillo. Por tanto, el sistema sigue funcionando aunque haya fallos o cortes de red entre nodos.\n",
    "    * Disponibilidad (A): Siempre responde a las peticiones (lecturas/escrituras), aunque algunos datos puedan no estar totalmente sincronizados.\n",
    "\n",
    "* ¬øQue no cumple siempre?:\n",
    "    * Consistencia (C): Se sacrifica parcialmente, ya que los datos pueden tardar un poco en propagarse entre nodos. Esto se conoce como consistencia eventual.\n",
    "\n",
    "* ¬øRealmente sirve Cassandra en este caso de uso?\n",
    "\n",
    "    * Seg√∫n el teorema CAP, Cassandra es adecuada para este caso porque en un sistema global y distribuido de datos s√≠smicos es m√°s importante no perder datos y mantener el servicio disponible que tener consistencia instant√°nea en todas las r√©plicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9448ed16",
   "metadata": {},
   "source": [
    "### ======\n",
    "### Ventajas\n",
    "### ======"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eaccc4",
   "metadata": {},
   "source": [
    "* Alta escalabilidad horizontal: permite a√±adir nodos f√°cilmente sin afectar la disponibilidad ni el rendimiento.\n",
    "\n",
    "* Alt√≠sima velocidad de escritura: ideal para registrar datos de sensores y eventos s√≠smicos en tiempo real.\n",
    "\n",
    "* Disponibilidad continua: sin un √∫nico punto de fallo gracias a su arquitectura distribuida y replicaci√≥n entre nodos.\n",
    "\n",
    "* Modelo flexible de datos: no se requiere esquema r√≠gido, como pasa en SQL. Cassandra puede adaptarse f√°cilmente a nuevos tipos de datos, por ejemplo a nuevos par√°metros geol√≥gicos que previamente no estaban definidos.\n",
    "\n",
    "* Replicaci√≥n geogr√°fica: los datos se podr√≠an replicarse entre regiones o continentes, √∫til para observatorios s√≠smicos internacionales.\n",
    "\n",
    "* Lecturas r√°pidas por clave: muy eficiente para consultas que acceden por identificadores concretos (ID de evento, zona, etc.).\n",
    "\n",
    "* Tolerancia a fallos: sigue funcionando incluso si algunos nodos o centros de datos quedan fuera de servicio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d8de5a",
   "metadata": {},
   "source": [
    "### =========\n",
    "### Desventajas\n",
    "### ========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fbb440",
   "metadata": {},
   "source": [
    "* No soporta transacciones complejas ni joins: En el momento que la consulta sea un poco m√°s compleja de lo habitual y requiriese un acceso multitabla.\n",
    "\n",
    "* Curva de aprendizaje: Requiere entender muy bien el modelo de datos y tener muy bien especificadas las consultas a resolver. Esto genera mucho debate a la hora de construir las tablas que ejecuten de manera √≥ptima la query, si se quiere evitar el allow filtering.\n",
    "\n",
    "* Consistencia eventual: Como se ha visto antes, puede haber peque√±os retrasos en la sincronizaci√≥n entre r√©plicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c288ea7",
   "metadata": {},
   "source": [
    "## 2. CONEXI√ìN Y CREACI√ìN DE LAS TABLAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e88e8",
   "metadata": {},
   "source": [
    "**¬°IMPORTANTE!**: Antes de ejecutar las siguientes celdas, debes de seguir todos los pasos indicados en el Readme, para habilitar el servidor de Cassandra. Este cuaderno debe de ser ejecutado con **Jupyter Notebooks en local**, y no en Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f55b337",
   "metadata": {},
   "source": [
    "### ==========================\n",
    "### Celda 1: Inicializaci√≥n y Conexi√≥n\n",
    "### =========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "030626c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intentando conectar a Cassandra en ['127.0.0.1']:9042...\n",
      "‚ùå Error de conexi√≥n (Intento 1/15): ('Unable to connect to any servers', {'127.0.0.1:9042': OperationTimedOut('errors=Timed out creating connection (5 seconds), last_host=None')})\n",
      "Esperando 10 segundos...\n",
      "‚ùå Error de conexi√≥n (Intento 2/15): ('Unable to connect to any servers', {'127.0.0.1:9042': OperationTimedOut('errors=Timed out creating connection (5 seconds), last_host=None')})\n",
      "Esperando 10 segundos...\n",
      "‚ùå Error de conexi√≥n (Intento 3/15): ('Unable to connect to any servers', {'127.0.0.1:9042': OperationTimedOut('errors=Timed out creating connection (5 seconds), last_host=None')})\n",
      "Esperando 10 segundos...\n",
      "‚ùå Error de conexi√≥n (Intento 4/15): ('Unable to connect to any servers', {'127.0.0.1:9042': OperationTimedOut('errors=Timed out creating connection (5 seconds), last_host=None')})\n",
      "Esperando 10 segundos...\n",
      "‚úÖ Conexi√≥n exitosa al cluster en el intento 5.\n",
      "üîë KEYSAPCE 'seismic_data' activo.\n",
      "Versi√≥n de Cassandra: 5.0.5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "from cassandra import OperationTimedOut\n",
    "\n",
    "# --- Configuraci√≥n ---\n",
    "CASSANDRA_HOSTS = ['127.0.0.1'] \n",
    "PORT = 9042\n",
    "KEYSPACE = 'seismic_data'\n",
    "\n",
    "# 1. Conexi√≥n al Cluster con reintentos para dar tiempo al contenedor a inicializarse\n",
    "cluster = None\n",
    "session = None\n",
    "RETRY_ATTEMPTS = 15\n",
    "RETRY_DELAY_SEC = 10 \n",
    "\n",
    "print(f\"Intentando conectar a Cassandra en {CASSANDRA_HOSTS}:{PORT}...\")\n",
    "\n",
    "for i in range(RETRY_ATTEMPTS):\n",
    "    try:\n",
    "        # Me conecto desde el host de WSL\n",
    "        cluster = Cluster(CASSANDRA_HOSTS, port=PORT) \n",
    "        session = cluster.connect()\n",
    "        print(f\"‚úÖ Conexi√≥n exitosa al cluster en el intento {i+1}.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error de conexi√≥n (Intento {i+1}/{RETRY_ATTEMPTS}): {e}\")\n",
    "        if i < RETRY_ATTEMPTS - 1:\n",
    "            print(f\"Esperando {RETRY_DELAY_SEC} segundos...\")\n",
    "            time.sleep(RETRY_DELAY_SEC)\n",
    "        else:\n",
    "            print(\"üö® Fallo al conectar despu√©s de varios intentos.\")\n",
    "            raise ConnectionError(\"No se pudo conectar al cluster de Cassandra.\")\n",
    "\n",
    "\n",
    "# 2. Establecer Keyspace y Verificar Versi√≥n\n",
    "if session:\n",
    "    # Crear Keyspace (se debe hacer en la sesi√≥n inicial)\n",
    "    session.execute(f\"\"\"\n",
    "        CREATE KEYSPACE IF NOT EXISTS {KEYSPACE}\n",
    "        WITH replication = {{'class': 'SimpleStrategy', 'replication_factor': '1'}}\n",
    "    \"\"\")\n",
    "    session.set_keyspace(KEYSPACE)\n",
    "    print(f\"üîë KEYSAPCE '{KEYSPACE}' activo.\")\n",
    "    \n",
    "    # Mostrar versi√≥n\n",
    "    rows = session.execute(\"SELECT release_version FROM system.local;\").one()\n",
    "    print(f\"Versi√≥n de Cassandra: {rows.release_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7cca3a",
   "metadata": {},
   "source": [
    "### =========================\n",
    "### Creaci√≥n de las 4 Tablas Optimizadas\n",
    "### ========================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d33b8f",
   "metadata": {},
   "source": [
    "A continuaci√≥n se procede a la creaci√≥n de las 4 tablas, una para cada una de las queies propuestas en la actividad\n",
    "\n",
    "Las consultas a realizar son las siguientes:\n",
    " - 5.a.i: Filtra terremotos por el a√±o del evento mayor de 2015.\n",
    " - 5.a.ii: Filtra terremotos en el pa√≠s donde ocurri√≥ y que empiece por: ‚ÄúJapa...‚Äù para el caso de \"Japan\".\n",
    " - 5.b.i: Consulta por un pa√≠s concreto donde se han producido terremotos, mostrando todos los terremotos con magnitud superior a 7.0.\n",
    " - 5.b.ii: Consulta por un pa√≠s concreto donde se han producido terremotos, incluyendo s√≥lo los que presenten riesgo potencial de tsunami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab74300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creando las 4 Tablas optimizadas para las consultas...\n",
      "Status=OK --> Las 4 Tablas han sido creadas/verificadas\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCreando las 4 Tablas optimizadas para las consultas...\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# TABLA 1: earthquakes_by_year_range (Optimiza: 5.a.i -> eq_id y A√±o > 2015)\n",
    "# Estrategia: Particionar por rango de a√±os (a partir de 2015) para escalar.\n",
    "# L√≥gica PK: year_pk = 'False' (si < 2015) o 'True' (si >= 2015).\n",
    "# Clustering Key 1: Permite el filtro de rango (event_year > 2015).\n",
    "# ----------------------------------------------------------------------\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS earthquakes_by_year_range (\n",
    "        year_pk boolean,      \n",
    "        event_year int,\n",
    "        eq_id text,\n",
    "        \n",
    "        event_time timestamp, \n",
    "        intensity_mmi decimal, \n",
    "        country text, \n",
    "        duration_sec int,\n",
    "        \n",
    "        PRIMARY KEY (year_pk, event_year, eq_id)\n",
    "    ) WITH CLUSTERING ORDER BY (event_year ASC, eq_id ASC);\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# TABLA 2: earthquakes_by_country_initial (Optimiza: 5.a.ii -> eq_id y Pa√≠s 'Japa...')\n",
    "# Estrategia: Particionar por la inicial del pa√≠s (A-Z) para balanceo de carga en != particiones.\n",
    "# L√≥gica PK: country_initial_pk = Primera letra del Pa√≠s (ej: 'J' para Japan).\n",
    "# Clustering Key 1: Permite el filtro de rango (>= 'Japa' AND < 'Japb').\n",
    "# -------------------------------------------------------------------------------\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS earthquakes_by_country_starting_with_prefix (\n",
    "        country_initial_pk text,\n",
    "        country text,\n",
    "        eq_id text,\n",
    "        \n",
    "        event_time timestamp, \n",
    "        intensity_mmi decimal,\n",
    "        duration_sec int,\n",
    "        \n",
    "        PRIMARY KEY (country_initial_pk, country, eq_id)\n",
    "    ) WITH CLUSTERING ORDER BY (country ASC, eq_id ASC);\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# TABLA 3: earthquakes_by_country_and_magnitude (Optimiza: 5.b.i -> Pa√≠s y Magnitud > 7.0)\n",
    "# Estrategia: La PK es el Pa√≠s, ya que la consulta es \"por un pa√≠s concreto\".\n",
    "# Clustering Key 1: Permite el filtro de rango (mw > 7.0).\n",
    "# Clustering Key 2: Ordenar por evento m√°s reciente.\n",
    "# ----------------------------------------------------------------------\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS earthquakes_by_country_and_magnitude (\n",
    "        country text,\n",
    "        mw decimal,\n",
    "        event_time timestamp,\n",
    "        \n",
    "        eq_id text,\n",
    "        intensity_mmi decimal,\n",
    "        duration_sec int,\n",
    "        \n",
    "        PRIMARY KEY (country, mw, event_time)\n",
    "    ) WITH CLUSTERING ORDER BY (mw DESC, event_time DESC); \n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# TABLA 4: tsunami_potential_by_country (Optimiza: 5.b.ii -> Pa√≠s y riesgo potencial=TRUE)\n",
    "# Estrategia: Partici√≥n Compuesta (Pa√≠s, Tsunami Boolean) para acceder directamente a TRUE/FALSE.\n",
    "# Clustering Key 1: Ordenar por evento m√°s reciente.\n",
    "# ----------------------------------------------------------------------\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tsunami_potential_by_country (\n",
    "        country text,\n",
    "        tsunami_potential boolean,\n",
    "        event_time timestamp,\n",
    "        \n",
    "        -- Datos Desnormalizados\n",
    "        eq_id text,\n",
    "        mw decimal,\n",
    "        intensity_mmi decimal,\n",
    "        \n",
    "        PRIMARY KEY ((country, tsunami_potential), event_time) -- PK Compuesta\n",
    "    ) WITH CLUSTERING ORDER BY (event_time DESC);\n",
    "\"\"\")\n",
    "\n",
    "print(\"Status=OK --> Las 4 Tablas han sido creadas/verificadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ee35f",
   "metadata": {},
   "source": [
    "## 3. INSERCI√ìN DE DATOS EN CASSANDRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd56d86e",
   "metadata": {},
   "source": [
    "Vamos a sacar nuestra lista de paises con pycountry, pero dejando 5 paises por defecto para poder hacer ejemplos controlados en el apartado 5.a.ii.\n",
    "\n",
    "Tambi√©n queremos que China aparezca al menos un 25% de las veces, dado que se va a usar como pa√≠s de ejemplo para el posterior UPDATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdafc4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['French Guiana', 'Barbados', 'Chequia', 'Cameroon', 'Canada', 'Australia', 'Brunei Darussalam', 'Andorra', 'Antarctica']\n"
     ]
    }
   ],
   "source": [
    "import pycountry\n",
    "import random\n",
    "\n",
    "def random_country(country_set):\n",
    "    countries = [c.name for c in pycountry.countries if c.name != \"Czechia\" and  c.name not in country_set]\n",
    "    return random.choice(countries)\n",
    "\n",
    "\n",
    "country_set = set(['Antarctica', 'Andorra', 'Chequia', 'Cameroon', 'Canada'])\n",
    "\n",
    "while len(country_set) < 9:\n",
    "    country_set.add(random_country(country_set=country_set))\n",
    "\n",
    "top_9_countries = list(country_set)\n",
    "print(top_9_countries)\n",
    "\n",
    "def get_country():\n",
    "    return random.choice(top_9_countries) if random.random() <= 0.75 else 'China'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d359de62",
   "metadata": {},
   "source": [
    "Creamos las funciones auxiliares que nos ayudar√°n a determinar las partition keys de las queries 5.a.i y 5.a.ii, definidas en las tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a140366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_pk(year):\n",
    "    return True if year >= 2015 else False\n",
    "\n",
    "def country_initial(country):\n",
    "    return country[0].upper() if isinstance(country, str) and country else \"X\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2856dd54",
   "metadata": {},
   "source": [
    "Ahora escogeremos 100 filas aleatorias del dataset *earthquake_data_tsunami.csv*, un dataset de Kaggle de tsunamis, prepararemos esos datos y los insertaremos en las 4 tablas de Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8588330e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status=OK --> 100 registros insertados en las 4 tablas adaptadas.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv(\"./data/earthquake_data_tsunami.csv\")\n",
    "df_sample = df.sample(n=100, random_state=42).reset_index(drop=True)  # 100 filas aleatorias del dataset\n",
    "\n",
    "# === Insertar registros adaptados a las 4 tablas ===\n",
    "for idx, row in df_sample.iterrows():\n",
    "    eq_id = f\"eq_{idx+1:03d}\"\n",
    "    event_year = int(row['Year']) if random.random() <= 0.5 else random.choice([2012, 2013, 2014])\n",
    "    event_time = datetime(event_year, int(row['Month']), 1)\n",
    "    intensity_mmi = float(row['mmi'])\n",
    "    country = get_country()\n",
    "    duration_sec = random.randint(10, 600)\n",
    "    mw = float(row['magnitude'])\n",
    "    tsunami = bool(row['tsunami'])\n",
    "\n",
    "    # --- Tabla 1: earthquakes_by_year_range ---\n",
    "    session.execute(\"\"\"\n",
    "        INSERT INTO earthquakes_by_year_range (year_pk, event_year, eq_id, event_time, intensity_mmi, country, duration_sec)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\", (year_pk(event_year), event_year, eq_id, event_time, intensity_mmi, country, duration_sec))\n",
    "\n",
    "    # --- Tabla 2: earthquakes_by_country_starting_with_prefix ---\n",
    "    country_initial_pk = country[0]\n",
    "    session.execute(\"\"\"\n",
    "        INSERT INTO earthquakes_by_country_starting_with_prefix (country_initial_pk, country, eq_id, event_time, intensity_mmi, duration_sec)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\", (country_initial(country), country, eq_id, event_time, intensity_mmi, duration_sec))\n",
    "\n",
    "    # --- Tabla 3: earthquakes_by_country_and_magnitude ---\n",
    "    session.execute(\"\"\"\n",
    "        INSERT INTO earthquakes_by_country_and_magnitude (country, mw, event_time, eq_id, intensity_mmi, duration_sec)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\", (country, mw, event_time, eq_id, intensity_mmi, duration_sec))\n",
    "\n",
    "    # --- Tabla 4: tsunami_potential_by_country ---\n",
    "    session.execute(\"\"\"\n",
    "        INSERT INTO tsunami_potential_by_country (country, tsunami_potential, event_time, eq_id, mw, intensity_mmi)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\", (country, tsunami, event_time, eq_id, mw, intensity_mmi))\n",
    "\n",
    "print(f\"Status=OK --> {df_sample.shape[0]} registros insertados en las 4 tablas adaptadas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187ede1",
   "metadata": {},
   "source": [
    "## 4. UPDATE EN CASSANDRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e128dd20",
   "metadata": {},
   "source": [
    "Elegimos la tabla *earthquakes_by_country_and_magnitude*, en la que vamos a actualizar un registro.\n",
    "\n",
    "Tal y como nos pone el enunciado, tenemos que actualizar los registros de un pa√≠s concreto, cambiando el nombre del pa√≠s a letras may√∫sculas. \n",
    "\n",
    " - Ej: China ‚Üí CHINA.\n",
    "\n",
    "Usaremos \"China\" como pa√≠s a actualizar, dado que en `get_country()` hemos puesto por defecto que retorne \"China\" un 30% de las veces, para estar bastante seguros que hay datos para actualizar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48e2b878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Registro encontrado: Row(country='China', mw=Decimal('8.6'), event_time=datetime.datetime(2005, 3, 1, 0, 0), eq_id='eq_048', intensity_mmi=Decimal('8.0'), duration_sec=443)\n",
      " -> Registro insertado con country='CHINA', eq_id=eq_048\n",
      " -> Registro eliminado para country='China', eq_id=eq_048\n",
      "\n",
      " -> Registros verificados con country='CHINA':\n",
      "Row(country='CHINA', eq_id='eq_048', mw=Decimal('8.6'), intensity_mmi=Decimal('8.0'), event_time=datetime.datetime(2005, 3, 1, 0, 0))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "country_used_to_delete = 'China'\n",
    "\n",
    "\n",
    "row = session.execute(\"\"\"\n",
    "    SELECT country, mw, event_time, eq_id, intensity_mmi, duration_sec\n",
    "    FROM earthquakes_by_country_and_magnitude\n",
    "    WHERE country = %s\n",
    "\"\"\", (country_used_to_delete,)).one()\n",
    "\n",
    "if not row:\n",
    "    print(f\"No se encontr√≥ ning√∫n registro con country='{country_used_to_delete}'\")\n",
    "else:\n",
    "    print(\" -> Registro encontrado:\", row)\n",
    "\n",
    "    mw = row.mw\n",
    "    event_time = row.event_time\n",
    "    eq_id = row.eq_id\n",
    "    intensity_mmi = row.intensity_mmi\n",
    "    duration_sec = row.duration_sec\n",
    "\n",
    "    # Insertamos el registro existente con el nuevo campo 'CHINA'\n",
    "    insert_cql = \"\"\"\n",
    "        INSERT INTO earthquakes_by_country_and_magnitude \n",
    "        (country, mw, event_time, eq_id, intensity_mmi, duration_sec)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    session.execute(insert_cql, ('CHINA', mw, event_time, eq_id, intensity_mmi, duration_sec))\n",
    "    print(f\" -> Registro insertado con country='CHINA', eq_id={eq_id}\")\n",
    "\n",
    "    # Borramos el registro antiguo, ya que UPDATE no borra\n",
    "    delete_cql = \"\"\"\n",
    "        DELETE FROM earthquakes_by_country_and_magnitude\n",
    "        WHERE country=%s AND mw=%s AND event_time=%s\n",
    "    \"\"\"\n",
    "    session.execute(delete_cql, (country_used_to_delete, mw, event_time))\n",
    "    print(f\" -> Registro eliminado para country='{country_used_to_delete}', eq_id={eq_id}\")\n",
    "\n",
    "    # Verificamos\n",
    "    result = session.execute(\"\"\"\n",
    "        SELECT country, eq_id, mw, intensity_mmi, event_time\n",
    "        FROM earthquakes_by_country_and_magnitude\n",
    "        WHERE country = %s\n",
    "    \"\"\", ('CHINA',))\n",
    "\n",
    "    print(\"\\n -> Registros verificados con country='CHINA':\")\n",
    "    for r in result:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe522ef",
   "metadata": {},
   "source": [
    "En este flujo podemos ver que nos encuentra un registro que contiene \"China\" en su atributo country, y se puede ver todo el flujo de ejecuci√≥n:\n",
    " * Primero encuentra un registro y guarda sus valores (el id, mw, event_time, ...)\n",
    "\n",
    " * Inserta un nuevo registro en la misma tabla, pero cambiando el atributo de country a \"CHINA\"\n",
    "\n",
    " * Elimina el registro seleccionado en el paso 1, para seguir la l√≥gica UPDATE de cambio completo de un registro\n",
    "\n",
    " * Por √∫ltimo, se verifica que el registro haya sido cambiado en el atributo country a CHINA, y que el resto de valores mantengan la consistencia con los valores que ten√≠a el antiguo registro eliminado\n",
    "\n",
    " En el output obtenido, se puede observar que sigue todos los pasos y la validaci√≥n es completamente correcta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bf24f9",
   "metadata": {},
   "source": [
    "## 5. CONSULTAS CQL Y PRUEBAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476174ba",
   "metadata": {},
   "source": [
    "### ===================\n",
    "### Definici√≥n de las Queries\n",
    "### ==================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec3c5b0",
   "metadata": {},
   "source": [
    "Se procede a la definici√≥n de las 4 queries. Ser√° una query por tabla, ya que en una base de datos NoSQL siempre se debe de plantear el modelo de datos en base a las consultas que se quieren realizar (modelo de datos hecho ya en el paso de creaci√≥n de las tablas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54583e27",
   "metadata": {},
   "source": [
    "Query 5.a.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40ad9943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_earthquake_by_year_range(session, year):  \n",
    "    query = \"\"\"\n",
    "    SELECT * FROM earthquakes_by_year_range \n",
    "    WHERE year_pk = ?  \n",
    "      AND event_year > ?\n",
    "    \"\"\"\n",
    "    prepared = session.prepare(query)\n",
    "\n",
    "    start_time = time.time()\n",
    "    rows = session.execute(prepared, (year_pk(year), year))\n",
    "    end_time = time.time()\n",
    "\n",
    "    time_ms = (end_time - start_time) * 1000\n",
    "\n",
    "    return rows, time_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b205e7",
   "metadata": {},
   "source": [
    "Query 5.a.ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d8443ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_prefix(prefix):\n",
    "    if not prefix:\n",
    "        return None\n",
    "    last_char = prefix[-1]\n",
    "    next_char = chr(ord(last_char) + 1)\n",
    "    return prefix[:-1] + next_char\n",
    "\n",
    "def query_earthquake_by_country_prefix(session, country_prefix):\n",
    "    c_i = country_initial(country_prefix)\n",
    "    upper_bound = get_next_prefix(country_prefix)\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT * FROM earthquakes_by_country_starting_with_prefix \n",
    "    WHERE country_initial_pk = ? \n",
    "      AND country >= ? \n",
    "      AND country < ?\n",
    "    \"\"\"\n",
    "    prepared = session.prepare(query)\n",
    "\n",
    "    start_time = time.time()\n",
    "    rows = session.execute(prepared, (c_i, country_prefix, upper_bound))\n",
    "    end_time = time.time()\n",
    "\n",
    "    time_ms = (end_time - start_time) * 1000\n",
    "\n",
    "    return rows, time_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a25575",
   "metadata": {},
   "source": [
    "Query 5.b.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7e6a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "def query_earthquakes_by_magnitude(session, country, min_magnitude):\n",
    "    query = \"\"\"\n",
    "    SELECT * FROM earthquakes_by_country_and_magnitude \n",
    "    WHERE country = ? \n",
    "      AND mw > ?\n",
    "    \"\"\"\n",
    "    prepared = session.prepare(query)\n",
    "\n",
    "    start_time = time.time()\n",
    "    rows = session.execute(prepared, (country, Decimal(str(min_magnitude))))\n",
    "    end_time = time.time()\n",
    "\n",
    "    time_ms = (end_time - start_time) * 1000\n",
    "\n",
    "    return rows, time_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a7d2de",
   "metadata": {},
   "source": [
    "Query 5.b.ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71a9f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_tsunami_potential_by_country(session, country_name):\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT * FROM tsunami_potential_by_country \n",
    "    WHERE country = ? \n",
    "      AND tsunami_potential = ?\n",
    "    \"\"\"\n",
    "    prepared = session.prepare(query)\n",
    "\n",
    "    start_time = time.time()\n",
    "    rows = session.execute(prepared, (country_name, True))\n",
    "    end_time = time.time()\n",
    "\n",
    "    time_ms = (end_time - start_time) * 1000\n",
    "\n",
    "    return rows, time_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5adfa",
   "metadata": {},
   "source": [
    "### ===================\n",
    "### Testeo de las Queries\n",
    "### ==================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85820cd8",
   "metadata": {},
   "source": [
    "#### 5.a.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68e71229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year_pk  event_year   eq_id        country  duration_sec event_time  \\\n",
      "0     True        2018  eq_016  French Guiana           383 2018-10-01   \n",
      "1     True        2018  eq_055       Cameroon           156 2018-08-01   \n",
      "2     True        2019  eq_043          China           467 2019-08-01   \n",
      "3     True        2019  eq_091         Canada           266 2019-08-01   \n",
      "4     True        2020  eq_093        Andorra            40 2020-10-01   \n",
      "\n",
      "  intensity_mmi  \n",
      "0           3.0  \n",
      "1           6.0  \n",
      "2           4.0  \n",
      "3           6.0  \n",
      "4           7.0  \n",
      "\n",
      "-> Tiempo de ejecuci√≥n: 7.929325103759766 ms; Registros totales obtenidos: 10\n",
      "\n",
      "-> Frecuencia de registros >= 2015: 10\n"
     ]
    }
   ],
   "source": [
    "year = 2017\n",
    "rows, elapsed_time = query_earthquake_by_year_range(session, year)\n",
    "\n",
    "df = pd.DataFrame(list(rows))\n",
    "\n",
    "if df.shape[0] > 0:\n",
    "    frec_yearPk = df['year_pk'].value_counts().values\n",
    "    print(df.head())\n",
    "    print(f\"\\n-> Tiempo de ejecuci√≥n: {elapsed_time} ms; Registros totales obtenidos: {df.shape[0]}\")\n",
    "    print(f'\\n-> Frecuencia de registros >= 2015: {df[df['year_pk'] == True].shape[0]}')\n",
    "\n",
    "else:\n",
    "    if year < 2015:\n",
    "        print(f\"No hay ning√∫n registro desde {year} hasta 2015\")\n",
    "    else:\n",
    "        print(f\"No hay ning√∫n registro >= {year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c0b9e",
   "metadata": {},
   "source": [
    "Se simula la query filtrando por un a√±o mayor o igual a 2015. Cabe destacar que la l√≥gica ha sido generalizada para soportar tambi√©n a√±os inferiores a 2015, buscando desde ese a√±o hasta 2015 (Ej: 2013 hasta 2015). Se puede probar con otro a√±o para validar varios casos y la tabla en cuesti√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6d33e",
   "metadata": {},
   "source": [
    "Si el a√±o es igual o superior a 2015 se observar√° un otuput con este formato:\n",
    "\n",
    "    -> Tiempo de ejecuci√≥n: 4.205226898193359 ms; Registros totales obtenidos: N\n",
    "\n",
    "    -> Frecuencia de registros >= 2015: N\n",
    "\n",
    "El n√∫mero total de registros siempre ser√° igual a la frecuencia de los registros mayores a 2015 --> N registros.\n",
    "\n",
    "Si el a√±o es inferior a 2015, se observar√° un output de este estilo:\n",
    "    \n",
    "    -> Tiempo de ejecuci√≥n: 4.205226898193359 ms; Registros totales obtenidos: K\n",
    "\n",
    "    -> Frecuencia de registros >= 2015: 0\n",
    "\n",
    "La frecuencia de los registros mayores a 2015 ser√° siempre 0, confirm√°ndonos que las particiones act√∫an de manera correcta, mandando los registros con a√±os >= 2015 a una partici√≥n y los inferiores a la otra partici√≥n, balanceando la carga.\n",
    "\n",
    "Adem√°s, podemos observar que en muy pocos minutos la query ha sido resuelta, indic√°ndonos que nuestras particiones y tablas est√°n bien optimizadas para esta consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35166063",
   "metadata": {},
   "source": [
    "#### 5.a.ii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f66807",
   "metadata": {},
   "source": [
    "Aqu√≠, se debe probar con varios prefijos. Yo he probado con los siguientes, ya que en los paises insertados existe probabilidad alta de que haya registros:\n",
    " * `Prefix: C --> Output: {Cameroon, China, Chequia, Canada, ...}`\n",
    " * `Prefix: Ch --> Output: {China, Chequia, ...}`\n",
    " * `Prefix: Ca --> Output: {Cameroon, Canada, ...}`\n",
    " * `Prefix: A --> Output: {Andorra, Antartica, ...}`\n",
    " * `Prefix: An --> Output: {Andorra, Antartica, ...}`\n",
    "\n",
    "Como m√≠nimo saldr√°n esos pa√≠ses al insertar esos prefijos, salvo que hayas tenido muy mala suerte y justo el random no te haya metido alguno de esos paises en ninguno de los 100 registros, pero eso es poco probable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2813fc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country_initial_pk  country   eq_id  duration_sec event_time intensity_mmi\n",
      "0                  A  Andorra  eq_019           549 2017-10-01           5.0\n",
      "1                  A  Andorra  eq_021           522 2014-05-01           4.0\n",
      "2                  A  Andorra  eq_037           296 2014-11-01           6.0\n",
      "3                  A  Andorra  eq_054           296 2012-09-01           5.0\n",
      "4                  A  Andorra  eq_066           368 2013-05-01           5.0\n",
      "\n",
      "-> Tiempo de ejecuci√≥n: 4.824161529541016 ms; Filas obtenidas: 22\n",
      "\n",
      "-> Frecuencias de cada pa√≠s:\n",
      "country\n",
      "Andorra       9\n",
      "Australia     8\n",
      "Antarctica    5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "rows, elapsed_time = query_earthquake_by_country_prefix(session, 'A')\n",
    "\n",
    "df = pd.DataFrame(list(rows))\n",
    "frecuencia_paises = df['country'].value_counts()\n",
    "print(df.head())\n",
    "print(f\"\\n-> Tiempo de ejecuci√≥n: {elapsed_time} ms; Filas obtenidas: {df.shape[0]}\")\n",
    "print('\\n-> Frecuencias de cada pa√≠s:')\n",
    "print(frecuencia_paises)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e04a39",
   "metadata": {},
   "source": [
    "Se puede observar que en cada partici√≥n van a estar agrupados pa√≠ses que tengan la mimsa inicial:\n",
    "\n",
    "    -> Prefix = 'An' --> 'Andorra', 'Antartica' --> Partici√≥n: \"A\"\n",
    "    -> Prefix = 'Ch' --> 'Chequia', 'Chile', 'China'  --> Partici√≥n:  \"C\"\n",
    "    -> Prefix = 'Chi' --> 'Chile', 'China'  --> Partici√≥n: \"C\"\n",
    "\n",
    "Se puede ver con esto que las particiones por iniciales funcionan bien para balancear la carga y que adem√°s sea capaz de buscar paises por prefijos en una misma partici√≥n.\n",
    "\n",
    "Tambi√©n se puede observar la actuaci√≥n de la Clustering Key en el *df.head*, dado que sacar√° los registros ordenados alfab√©ticamente por Country. Se debe a que en la creaci√≥n de la tabla *earthquakes_by_country_starting_with_prefix*, la primera Clustering Key es country."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22713a70",
   "metadata": {},
   "source": [
    "#### 5.b.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fdade2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country   mw event_time  duration_sec   eq_id intensity_mmi\n",
      "0   China  7.4 2014-06-01           518  eq_084           7.0\n",
      "1   China  7.4 2013-03-01           143  eq_028           8.0\n",
      "2   China  7.4 2003-09-01            46  eq_007           7.0\n",
      "\n",
      "-> Tiempo de ejecuci√≥n: 6.452322006225586 ms; Filas obtenidas: 3\n",
      "\n",
      "-> Frecuencia escalas superiores a 7.0:\n",
      "country  mw   event_time  duration_sec  eq_id   intensity_mmi\n",
      "China    7.4  2003-09-01  46            eq_007  7.0              1\n",
      "              2013-03-01  143           eq_028  8.0              1\n",
      "              2014-06-01  518           eq_084  7.0              1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "mw = 7.1\n",
    "rows, elapsed_time = query_earthquakes_by_magnitude(session, 'China', mw)\n",
    "\n",
    "df = pd.DataFrame(list(rows))\n",
    "\n",
    "if df.shape[0] > 0:\n",
    "    frecu_escala = df[df['mw'] >= 7.0].value_counts()\n",
    "    print(df.head())\n",
    "    print(f\"\\n-> Tiempo de ejecuci√≥n: {elapsed_time} ms; Filas obtenidas: {df.shape[0]}\")\n",
    "    print(f'\\n-> Frecuencia escalas superiores a 7.0:')\n",
    "    print(frecu_escala)\n",
    "else:\n",
    "    print(f\"No hay escala disponible a partir de {mw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b052498",
   "metadata": {},
   "source": [
    "En este filtro, si se prueba a ejecutar con distintos paises y distintas escalas de magnitud, se puede observar que algunos de los pa√≠ses no contienen magnitudes superiores a 7.0\n",
    "\n",
    "De acuerdo al dataset descargado de Kaggle, la mayor√≠a de las magnitudes no son superiores a 7, y al recolectar esos datos de ese dataset, y no haberle aplicado al *mw* ninguna modificaci√≥n, va a tener esta consecuencia (que tiene sentido ya que una magnitud superior a 7 es bastante catastr√≥fica y no pasa regularmente en la vida real).\n",
    "\n",
    "Tambi√©n podremos observar (en el caso en el que si que existan datos), que la frecuencia ser√° baja, lo que indica consistencia con la vida real y con el dataset, dado que son magnitudes peligros√≠simas que ocurren muy poco en el planeta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e70dd93",
   "metadata": {},
   "source": [
    "#### 5.b.ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4870bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country  tsunami_potential event_time   eq_id intensity_mmi   mw\n",
      "0  Cameroon               True 2022-03-01  eq_025           4.0  6.9\n",
      "1  Cameroon               True 2021-08-01  eq_081           4.0  6.9\n",
      "2  Cameroon               True 2018-08-01  eq_055           6.0  7.3\n",
      "3  Cameroon               True 2017-03-01  eq_074           7.0  6.6\n",
      "4  Cameroon               True 2014-07-01  eq_039           4.0  6.5\n",
      "\n",
      "-> Tiempo de ejecuci√≥n: 5.401849746704102 ms; Filas obtenidas: 6\n",
      "\n",
      "-> Frecuencia de riesgos potenciales de sunami en Cameroon: 6\n"
     ]
    }
   ],
   "source": [
    "country = 'Cameroon'\n",
    "rows, elapsed_time = query_tsunami_potential_by_country(session, country)\n",
    "\n",
    "df = pd.DataFrame(list(rows))\n",
    "\n",
    "if df.shape[0] > 0:\n",
    "    print(df.head())\n",
    "    print(f\"\\n-> Tiempo de ejecuci√≥n: {elapsed_time} ms; Filas obtenidas: {df.shape[0]}\")\n",
    "    print(f'\\n-> Frecuencia de riesgos potenciales de sunami en {country}: {df[df['tsunami_potential'] == True].shape[0]}')\n",
    "else:\n",
    "    print(f\"{country} no tiene riesgo potencial de tsunami\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c869843b",
   "metadata": {},
   "source": [
    "Recordemos que en esta consulta, la partition key est√° compuesta con el pa√≠s y el riesgo de tsunami, ya que as√≠ podemos buscar en la misma partici√≥n de una manera muy r√°pida, el pa√≠s que tenga riesgo de tsunami, y a parte balanceando mucho m√°s la carga dado que los registro de ese mismo pa√≠s que no tienen rsiego de sunami van a ir a otra partici√≥n.\n",
    "\n",
    "Como se puede evaluar en la frecuencia de riesgos de tsunami en el filtro, coincide con el total de registros obtenidos, lo que nos indica que la consulta es consistente, ya que solo queremos sacar los riesgos potenciales de dicho pa√≠s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cdee4f",
   "metadata": {},
   "source": [
    "#### Evaluaci√≥n de tiempos de las consultas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a9b8c9",
   "metadata": {},
   "source": [
    "En los outputs obtenidos tras ejecutar las consultas, podemos observar que el tiempo de ejecuci√≥n de las queries en Cassandra dura entre 6 y 7 milisegundos. Esto demuestra la rapidez de las consultas, por lo que para un case de uso como el de los terremotos, en el que se aplique el Real Time con la recolecci√≥n de datos, unos tiempos de lectura y escritura a base de datos que sean muy r√°pidos es crucial.\n",
    "\n",
    "En t√©rminos de balanceo de carga de datos, se logra repartir de forma consistente en distintas particiones de acuerdo a las consultas definidas, y sin lugar a error. Distribuir la carga implicar√° aumentar la velocidad en el momento en el que nuevos datos sean introducidos, y tambi√©n mantener la consistencia y coherencia a la hora de sacar los resultados, sin tener que hacer uso del ALLOW FILTERING."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077f625",
   "metadata": {},
   "source": [
    "## 6. CERRAR SESI√ìN EN CASSANDRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98615d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd67dc",
   "metadata": {},
   "source": [
    "Una vez se acabe todo el flujo de sesi√≥n, desconecta el cluster para no tener muchas sesiones abiertas.\n",
    "\n",
    "Si se quiere volver a re ejecutar otra vez alguna celda concreta, tienes que volver a ejecutar la celda 1 de Inicializaci√≥n y Conexi√≥n."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cassandra-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
